{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucket ë¶„ë¦¬ ë° ê²€ìˆ˜ ì›Œí¬í”Œë¡œìš°\n",
    "\n",
    "## ì›Œí¬í”Œë¡œìš°\n",
    "1. JSON íŒŒì¼ ë¡œë“œ â†’ Bucketë³„ ë¶„ë¦¬ (A, B, C)\n",
    "2. ê° íŒŒì¼ ê²€ìˆ˜ (A: íŒ¨ìŠ¤, B: ê°„ë‹¨ ê²€í† , C: ì •ë°€ ê²€ìˆ˜)\n",
    "3. ê²€ìˆ˜ ì™„ë£Œ í›„ ë³‘í•© â†’ `text_raw` â†’ `text_verified` ë³€í™˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Google Drive ë§ˆìš´íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì • (ë³¸ì¸ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •)\n",
    "WORK_DIR = '/content/drive/MyDrive/TAPS'  # ìˆ˜ì • í•„ìš”\n",
    "\n",
    "import os\n",
    "os.makedirs(WORK_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: JSON íŒŒì¼ ë¡œë“œ ë° Bucketë³„ ë¶„ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_json(filepath):\n",
    "    \"\"\"JSON íŒŒì¼ ë¡œë“œ\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(data, filepath):\n",
    "    \"\"\"JSON íŒŒì¼ ì €ì¥\"\"\"\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"ì €ì¥ ì™„ë£Œ: {filepath} ({len(data)}ê°œ í•­ëª©)\")\n",
    "\n",
    "def split_by_bucket(data):\n",
    "    \"\"\"Bucketë³„ë¡œ ë°ì´í„° ë¶„ë¦¬\"\"\"\n",
    "    buckets = defaultdict(list)\n",
    "    for item in data:\n",
    "        bucket = item.get('bucket', 'unknown')\n",
    "        buckets[bucket].append(item)\n",
    "    return dict(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì›ë³¸ íŒŒì¼ ê²½ë¡œ ì„¤ì • (ìˆ˜ì • í•„ìš”)\n",
    "TRAIN_FILE = f\"{WORK_DIR}/asr_results_train_retriaged.json\"\n",
    "VAL_FILE = f\"{WORK_DIR}/asr_results_validation_retriaged.json\"  # íŒŒì¼ëª… í™•ì¸ í•„ìš”\n",
    "\n",
    "# íŒŒì¼ ë¡œë“œ\n",
    "train_data = load_json(TRAIN_FILE)\n",
    "print(f\"Train ë°ì´í„°: {len(train_data)}ê°œ\")\n",
    "\n",
    "# Validation íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ\n",
    "try:\n",
    "    val_data = load_json(VAL_FILE)\n",
    "    print(f\"Validation ë°ì´í„°: {len(val_data)}ê°œ\")\n",
    "except FileNotFoundError:\n",
    "    val_data = []\n",
    "    print(\"Validation íŒŒì¼ ì—†ìŒ (ìŠ¤í‚µ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucketë³„ ë¶„ë¦¬\n",
    "train_buckets = split_by_bucket(train_data)\n",
    "val_buckets = split_by_bucket(val_data) if val_data else {}\n",
    "\n",
    "print(\"\\n=== Train ë¶„í¬ ===\")\n",
    "for bucket, items in sorted(train_buckets.items()):\n",
    "    print(f\"  Bucket {bucket}: {len(items)}ê°œ\")\n",
    "\n",
    "if val_buckets:\n",
    "    print(\"\\n=== Validation ë¶„í¬ ===\")\n",
    "    for bucket, items in sorted(val_buckets.items()):\n",
    "        print(f\"  Bucket {bucket}: {len(items)}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucketë³„ íŒŒì¼ ì €ì¥\n",
    "OUTPUT_DIR = f\"{WORK_DIR}/bucket_review\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Train ë¶„ë¦¬ ì €ì¥\n",
    "for bucket, items in train_buckets.items():\n",
    "    save_json(items, f\"{OUTPUT_DIR}/train_bucket_{bucket}.json\")\n",
    "\n",
    "# Validation ë¶„ë¦¬ ì €ì¥\n",
    "for bucket, items in val_buckets.items():\n",
    "    save_json(items, f\"{OUTPUT_DIR}/val_bucket_{bucket}.json\")\n",
    "\n",
    "print(f\"\\nëª¨ë“  íŒŒì¼ì´ {OUTPUT_DIR}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: ê²€ìˆ˜ (ìˆ˜ë™ ì‘ì—…)\n",
    "\n",
    "ë¶„ë¦¬ëœ íŒŒì¼ë“¤ì„ ê²€ìˆ˜í•©ë‹ˆë‹¤:\n",
    "- **Bucket A**: ê²€ìˆ˜ ë¶ˆí•„ìš” (ê·¸ëŒ€ë¡œ ì‚¬ìš©)\n",
    "- **Bucket B**: ê°„ë‹¨íˆ í›‘ì–´ë³´ê¸° (í•„ìš”ì‹œ `text_raw` ìˆ˜ì •)\n",
    "- **Bucket C**: ì •ë°€ ê²€ìˆ˜ (`text_raw` ì§ì ‘ ìˆ˜ì •)\n",
    "\n",
    "### ê²€ìˆ˜ ë°©ë²•\n",
    "1. Google Driveì—ì„œ í•´ë‹¹ JSON íŒŒì¼ ë‹¤ìš´ë¡œë“œ\n",
    "2. í…ìŠ¤íŠ¸ ì—ë””í„°ë‚˜ JSON ì—ë””í„°ë¡œ ì—´ê¸°\n",
    "3. `text_raw` í•„ë“œ ì§ì ‘ ìˆ˜ì •\n",
    "4. ì €ì¥ í›„ ë‹¤ì‹œ ì—…ë¡œë“œ\n",
    "\n",
    "ë˜ëŠ” ì•„ë˜ ì…€ì—ì„œ ê°„ë‹¨í•œ ê²€ìˆ˜ ë„êµ¬ ì‚¬ìš©:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°„ë‹¨í•œ ê²€ìˆ˜ ë„êµ¬ (ì„ íƒì‚¬í•­)\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def review_bucket(filepath):\n",
    "    \"\"\"Bucket íŒŒì¼ ê²€ìˆ˜ ë„êµ¬\"\"\"\n",
    "    data = load_json(filepath)\n",
    "    current_idx = [0]  # mutable container for closure\n",
    "    \n",
    "    # Widgets\n",
    "    output = widgets.Output()\n",
    "    text_input = widgets.Textarea(\n",
    "        description='text_raw:',\n",
    "        layout=widgets.Layout(width='100%', height='100px')\n",
    "    )\n",
    "    idx_label = widgets.Label()\n",
    "    \n",
    "    def update_display():\n",
    "        item = data[current_idx[0]]\n",
    "        idx_label.value = f\"[{current_idx[0]+1}/{len(data)}] {item['speaker_id']}_{item['sentence_id']}\"\n",
    "        text_input.value = item['text_raw']\n",
    "        with output:\n",
    "            clear_output()\n",
    "            print(f\"bucket: {item['bucket']} | reason: {item['reason']}\")\n",
    "            print(f\"duration: {item['duration']:.2f}s | logprob: {item['avg_logprob']:.3f}\")\n",
    "    \n",
    "    def save_current(_):\n",
    "        data[current_idx[0]]['text_raw'] = text_input.value\n",
    "        print(\"ì €ì¥ë¨\")\n",
    "    \n",
    "    def next_item(_):\n",
    "        save_current(None)\n",
    "        if current_idx[0] < len(data) - 1:\n",
    "            current_idx[0] += 1\n",
    "            update_display()\n",
    "    \n",
    "    def prev_item(_):\n",
    "        save_current(None)\n",
    "        if current_idx[0] > 0:\n",
    "            current_idx[0] -= 1\n",
    "            update_display()\n",
    "    \n",
    "    def save_file(_):\n",
    "        save_current(None)\n",
    "        save_json(data, filepath)\n",
    "    \n",
    "    # Buttons\n",
    "    prev_btn = widgets.Button(description='â—€ ì´ì „')\n",
    "    next_btn = widgets.Button(description='ë‹¤ìŒ â–¶')\n",
    "    save_btn = widgets.Button(description='ğŸ’¾ íŒŒì¼ ì €ì¥', button_style='success')\n",
    "    \n",
    "    prev_btn.on_click(prev_item)\n",
    "    next_btn.on_click(next_item)\n",
    "    save_btn.on_click(save_file)\n",
    "    \n",
    "    update_display()\n",
    "    \n",
    "    display(widgets.VBox([\n",
    "        idx_label,\n",
    "        output,\n",
    "        text_input,\n",
    "        widgets.HBox([prev_btn, next_btn, save_btn])\n",
    "    ]))\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ (ê²€ìˆ˜í•  íŒŒì¼ ê²½ë¡œ ì…ë ¥)\n",
    "# review_bucket(f\"{OUTPUT_DIR}/train_bucket_C.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: ê²€ìˆ˜ ì™„ë£Œ í›„ ë³‘í•©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_convert(bucket_files, output_path):\n",
    "    \"\"\"\n",
    "    Bucket íŒŒì¼ë“¤ì„ ë³‘í•©í•˜ê³  text_raw â†’ text_verifiedë¡œ ë³€í™˜\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    \n",
    "    for filepath in bucket_files:\n",
    "        if os.path.exists(filepath):\n",
    "            data = load_json(filepath)\n",
    "            merged.extend(data)\n",
    "            print(f\"ë¡œë“œ: {filepath} ({len(data)}ê°œ)\")\n",
    "        else:\n",
    "            print(f\"íŒŒì¼ ì—†ìŒ (ìŠ¤í‚µ): {filepath}\")\n",
    "    \n",
    "    # text_raw â†’ text_verified ë³€í™˜\n",
    "    for item in merged:\n",
    "        item['text_verified'] = item.pop('text_raw')\n",
    "    \n",
    "    # index ì¬ì •ë ¬ (ì„ íƒì‚¬í•­)\n",
    "    merged.sort(key=lambda x: (x['speaker_id'], x['sentence_id']))\n",
    "    \n",
    "    save_json(merged, output_path)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ë³‘í•©\n",
    "train_bucket_files = [\n",
    "    f\"{OUTPUT_DIR}/train_bucket_A.json\",\n",
    "    f\"{OUTPUT_DIR}/train_bucket_B.json\",\n",
    "    f\"{OUTPUT_DIR}/train_bucket_C.json\",\n",
    "]\n",
    "\n",
    "train_merged = merge_and_convert(\n",
    "    train_bucket_files,\n",
    "    f\"{WORK_DIR}/train_verified.json\"\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain ë³‘í•© ì™„ë£Œ: {len(train_merged)}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation ë³‘í•© (ìˆëŠ” ê²½ìš°)\n",
    "val_bucket_files = [\n",
    "    f\"{OUTPUT_DIR}/val_bucket_A.json\",\n",
    "    f\"{OUTPUT_DIR}/val_bucket_B.json\",\n",
    "    f\"{OUTPUT_DIR}/val_bucket_C.json\",\n",
    "]\n",
    "\n",
    "# íŒŒì¼ì´ í•˜ë‚˜ë¼ë„ ì¡´ì¬í•˜ë©´ ë³‘í•©\n",
    "if any(os.path.exists(f) for f in val_bucket_files):\n",
    "    val_merged = merge_and_convert(\n",
    "        val_bucket_files,\n",
    "        f\"{WORK_DIR}/val_verified.json\"\n",
    "    )\n",
    "    print(f\"\\nValidation ë³‘í•© ì™„ë£Œ: {len(val_merged)}ê°œ\")\n",
    "else:\n",
    "    print(\"Validation bucket íŒŒì¼ ì—†ìŒ (ìŠ¤í‚µ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: ê²°ê³¼ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ê²°ê³¼ ìƒ˜í”Œ í™•ì¸\n",
    "final_train = load_json(f\"{WORK_DIR}/train_verified.json\")\n",
    "\n",
    "print(\"=== ìƒ˜í”Œ í™•ì¸ ===\")\n",
    "for item in final_train[:3]:\n",
    "    print(f\"\\n{item['speaker_id']}_{item['sentence_id']}:\")\n",
    "    print(f\"  text_verified: {item['text_verified']}\")\n",
    "    print(f\"  bucket: {item['bucket']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "ê²€ìˆ˜ ì™„ë£Œëœ `train_verified.json`, `val_verified.json` íŒŒì¼ì„ normalizerì— ì ìš©:\n",
    "\n",
    "```python\n",
    "from taps.normalizer import normalize\n",
    "\n",
    "for item in data:\n",
    "    item['text_normalized'] = normalize(item['text_verified'])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
