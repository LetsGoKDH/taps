# 3.1 라벨링 자동화 워크플로우 조사 및 베이스라인 결정 (TAPS Train/Dev)

## 1. 목적과 전제

### 목적
TAPS 데이터셋의 **Train/Dev split에서 비어 있는 `text`를 자동으로 생성**하고, 이를 기반으로 **`normalized text`를 일관되게 생성**하기 위한 라벨링 자동화 워크플로우를 조사·비교한 뒤, 프로젝트의 **베이스라인 워크플로우**를 확정한다.

### 전제(Assumptions)
- Train/Dev의 `text`는 비어 있으며, 프로젝트에서 이를 생성해야 한다.
- Throat/Acoustic은 **동일 발화(동일 문장)를 두 채널로 녹음한 페어**이며, **최종 텍스트 라벨은 채널 간 공유**한다.
- 본 단계(3.1)에서는 "데이터 증강/보강"은 범위 밖이며, **일반적인 음성→텍스트 라벨링**에 집중한다.
- 실행 환경은 **로컬 노트북 또는 Google Colab**에서 무리 없이 구동 가능해야 한다.

---

## 2. 후보 워크플로우 분류

본 조사에서는 음성-텍스트 라벨링 자동화 접근을 다음 3가지 계열로 분류했다.

### A) 딥러닝 기반 자동 전사(ASR) 중심
- 예: Whisper 계열, Wav2Vec2 계열, 기타 오픈소스 ASR
- 음성 입력 → 텍스트 출력(문장 단위 전사 생성)

### B) 규칙 기반/강제정렬(Forced Alignment) 중심
- 예: Montreal Forced Aligner(MFA) 등
- **텍스트가 이미 존재**할 때: 음성-텍스트를 시간축으로 정렬/검증(텍스트 "생성"이 아니라 "정렬")

### C) 하이브리드(ASR + 트리아지 + 최소 수동검수 + 규칙 정규화)
- ASR로 초안 생성 후, 자동 점수화로 검수 대상만 선별하고, 규칙 기반 정규화를 통해 재현성을 확보

---

## 3. 워크플로우 비교(장단점 및 적합성)

| 워크플로우 | 개요 | 장점 | 단점/리스크 | TAPS(Train/Dev text 비어 있음) 적합성 |
|---|---|---|---|---|
| A) ASR 단독(완전 자동) | Acoustic(또는 둘 다)에 ASR 실행 → 결과를 정답으로 고정 | 구현 단순, 빠른 처리, Colab 친화적 | 오인식이 그대로 "정답 라벨"로 고정될 위험. 품질 관리 부재 | **중간** (빠르지만 품질 담보 어려움) |
| B) Forced Alignment 단독 | 텍스트가 있을 때만 음성에 정렬(시간정보/검증) | 텍스트가 정확하면 정렬 품질 높음, 시간 라벨 생성 용이 | **텍스트가 없으면 시작 불가**. 현재 과제(텍스트 생성)와 불일치 | **낮음** (현 단계 핵심 문제 해결 불가) |
| C) 하이브리드(추천) | **Acoustic-only ASR → 자동 트리아지 → 필요한 샘플만 검수 → 규칙 정규화** | 품질/효율 균형. "정답 라벨"로서의 신뢰성 확보. 정규화는 결정적(deterministic)으로 재현성 확보 | 구성요소가 늘어 구현이 약간 복잡. 트리아지 기준 설계 필요 | **높음** (텍스트 생성 문제를 직접 해결 + 품질 관리 가능) |

**핵심 결론:**
Train/Dev의 `text`가 비어 있는 상황에서는 **텍스트 생성이 선결 과제**이므로, 정렬 도구(Forced Alignment)는 3.1의 베이스라인으로 부적합하다고 여겼다. 따라서 **ASR 기반 생성**이 필수이며, "정답 라벨 고정"을 위해서는 **품질 트리아지 + 최소 검수 + 결정적 정규화**를 포함하는 하이브리드가 최적이라 판단 내렸다.

---

## 4. 베이스라인 워크플로우 결정(3.1 최종안)

### 4.1 정책(Policy): 텍스트 라벨의 단위
- 최종 텍스트 라벨은 **sample_id(문장) 단위로 1개만 존재**한다.
- Throat/Acoustic은 동일 발화이므로 동일 텍스트를 **공유**한다.
- 따라서 텍스트 생성은 **Acoustic 기준**으로 수행하기로 결정했다(품질/일반성 측면에서 유리).

### 4.2 산출물(Artifacts): 텍스트 3종 분리
아래 3개 필드를 **명확히 분리하여 관리**한다.

1) `text_raw`: ASR이 출력한 원문(자동 생성 결과, 변경 최소화)
2) `text_verified`: "정답 라벨"로 확정한 전사 텍스트(필요 시 최소 수정 반영)
3) `text_normalized`: 학습/평가에 사용할 정규화 텍스트(규칙 기반, 재현성 확보)

> 원칙: `text_raw`는 최대한 보존하고, 사람이 고치는 것은 `text_verified`에서만 수행한다. `text_normalized`는 항상 `text_verified → normalize()`로 생성한다.

### 4.3 파이프라인(Workflow)
**Step 0. 입력 단위 확정**
- 입력은 "샘플 ID(문장)"이며, Acoustic 오디오 경로를 기준으로 처리한다.

**Step 1. Acoustic-only ASR 실행 → `text_raw` 생성**
- Colab/로컬에서 재현 가능한 ASR 엔진 1개를 베이스로 고정한다.
- 실행 파라미터(모델 크기, 디코딩 옵션 등)는 문서/코드로 고정한다.
- **선정 모델**: Whisper Large-v3 (상세 근거는 4.5절 참조)

**Step 2. 자동 품질 트리아지(Triage)**
- 모든 샘플에 대해 간단한 proxy 지표로 신뢰도를 점수화하고, A/B/C로 버킷팅한다.
  - A(High): 자동 확정 후보
  - B(Medium): 빠른 검수 대상
  - C(Low): 집중 검수/보류 대상

> 목적: 전체를 수동 검수하지 않으면서도 "정답 라벨로서의 품질"을 확보한다.

> 참고: 트리아지 단계는 규칙 기반 proxy 지표로 구성하되, 필요 시 LLM을 **검수 우선순위/의심 사유 제안** 역할로만 옵션 플러그인 형태로 추가할 수 있다(자동 확정 금지).

**Step 3. (필요 샘플만) 최소 수동 검수 → `text_verified` 확정**
- 기본 정책: A는 `text_verified = text_raw`로 두고, B/C 위주로만 듣고 수정한다.
- 검수 결과는 "수정 로그/근거"를 남겨 재현 가능하게 한다.

**Step 4. 규칙 기반 정규화 → `text_normalized` 생성**
- 정규화는 반드시 결정적(deterministic) 규칙/모듈로 수행한다.
- 숫자/단위/기호/공백/한글화 등 한국어 도메인 규칙은 3.2에서 정리하고 3.3~에서 확장한다.

---

## 4.4 ASR 모델 선정 근거

TAPS 데이터셋에 대한 라벨링 자동화를 위해 여러 한국어 ASR 모델의 성능을 비교 평가하였다. 평가는 TAPS 데이터셋의 **Test split(약 1,000 문장)**을 대상으로 수행하였으며, **문자 오류율(CER, Character Error Rate)**을 주요 지표로 사용했다. 평가 시 별도의 텍스트 후처리(정규화)는 적용하지 않았다.

### 비교 결과

| 모델 (설정) | CER |
|---|---|
| **Whisper Large-v3 (beam=5)** | **6.71%** |
| Whisper Large-v3 (beam=10) | 6.72% |
| Whisper Large-v3 (beam=1, greedy) | 6.78% |
| Whisper Large-v3 fine-tuned (Zeroth-KO v2) | 10.47% |
| Wav2Vec2 Large XLSR-Korean | 20.43% |
| Wav2Vec2 300M + KenLM (beam=100) | 21.67% |

### 평가 설정
- **모델**: `Systran/faster-whisper-large-v3` (OpenAI Whisper Large-v3의 최적화 구현)
- **디코딩 파라미터**:
  - `language="ko"`
  - `beam_size=5` (최종 선정)
  - `temperature=[0.0, 0.2, 0.4]`
  - `compute_type="int8_float16"`
- **평가 데이터**: TAPS Test split (~1,000 samples)
- **평가 지표**: Raw CER (정규화 미적용)

### 선정 근거

Whisper Large-v3 (beam=5)가 **CER 6.71%**로 가장 우수한 성능을 보였다:

1. **압도적 정확도**: 동일 계열 fine-tuned 모델(10.47%) 대비 40% 이상 오류 감소
2. **범용성**: 약 500만 시간 이상의 다국어 데이터로 학습되어 다양한 도메인에 강건함
3. **디코딩 효율성**: beam=5에서 최적 성능 (beam=10과 거의 동일하나 연산 효율적)
4. **재현성**: faster-whisper 구현으로 Colab/로컬 환경에서 안정적 재현 가능

Whisper Large-v3를 사용함으로써 Train/Dev 세트의 자동 전사 품질을 최대화하고, 수동 검수 부담을 최소화할 수 있다.

---

## 4.5 한국어 도메인 적용 방법(3.1 단계에서의 "방법" 결정)

3.1의 범위는 **한국어 규칙의 '세부 목록'을 완성(=3.2)** 하는 것이 아니라,
향후 3.2~3.6에서 확장 가능한 형태로 **"한국어 도메인 적용 방식(아키텍처/정책/인터페이스)"을 결정**하는 것이다.

### 결정 1) 정규화는 '결정적(deterministic)' 규칙 기반으로 구현한다
- 동일 입력은 항상 동일 출력이 되도록 한다.
- LLM(대규모 언어 모델)은 **기본 경로(default path)에서 '자동 확정' 용도로는 사용하지 않는다.**
  - 이유: 정답 라벨 파이프라인의 핵심 요구사항(재현성, 비가역 변형 최소화)을 해칠 수 있음.
- 다만 확장성을 위해 **옵션 플러그인(optional plugin)** 으로는 허용한다(아래 역할에 한정).
  - (A) 트리아지/오류 탐지 보조: 검수 우선순위 및 "의심 사유" 제안
  - (B) 후보 생성(Top-k rewrite) + 사람 선택: 자동 확정 금지, human gate 필수
  - (C) 규칙 제안: 케이스 분석/규칙 후보를 제안하고, 최종 적용은 규칙+테스트로 흡수
- 옵션 플러그인을 사용할 경우에도 아래를 **스펙으로 고정**한다.
  - 모델/버전 및 프롬프트 버전 기록
  - 가능하면 temperature=0
  - LLM 출력은 `text_verified`를 **직접 덮어쓰지 않음**(별도 필드에 저장 후 검수)

- 정규화는 항상 `text_verified → text_normalized`의 단방향 파이프라인으로 고정한다.

### 결정 2) 한국어 도메인 규칙은 "모듈화 + 테스트 주도"로 확장한다
- 규칙은 하나의 거대 함수로 누적하지 않고, 다음과 같이 **룰셋 단위로 분리**한다.
  - 숫자/서수/단위(예: 3km, 7일, 1개)
  - 기호/구두점/따옴표/괄호
  - 영어/약어/알파벳+숫자 혼용(예: AI, GPU, BERT, V2)
  - 고유명사/외래어 처리(정책에 따른 보수적 처리)
  - 띄어쓰기 정책(필요 시 제한적 적용)
- 각 룰셋은 **테스트 케이스(입력→기대 출력)와 함께** 추가한다(회귀 방지).
- 룰 적용 순서는 고정하고 문서화한다("파이프라인 순서도"가 스펙).

### 결정 3) "보수적 정규화"를 기본 정책으로 채택한다
정답 라벨 생성 단계에서는 과도한 추론/교정을 피하고, 의미 변형 위험이 낮은 변환을 우선한다.

- 기본 수행(권장):
  - 공백 정리(연속 공백 축약, 양끝 trim)
  - 불필요한 구두점/기호 정리(정책 기반)
  - 숫자/단위의 표기 통일(정책 기반)
- 선택 수행(3.2 이후 근거가 축적되면 활성화):
  - 공격적인 띄어쓰기 교정
  - 문법/맞춤법 교정
  - 의미 추론이 필요한 변환(예: 약어 확장)

### 결정 4) 한국어 도메인 적용의 "입력/출력 계약(contract)"을 고정한다
- 입력: `text_verified` (UTF-8, 문장 단위 문자열)
- 출력: `text_normalized` (동일)
- 추가 메타데이터(권장):
  - `norm_version` (정규화 버전)
  - `norm_notes` (룰 적용 요약/경고 플래그)
  - `norm_flags` (예: 숫자 포함, 영어 포함, 단위 포함 등)

> 세부 규칙 목록과 예외 케이스(숫자 읽기, 단위 결합, '한/일' 등)는 3.2에서 체계적으로 수집·정리하고, 3.3~3.6에서 구현/검증한다.

## 5. 트리아지 설계(초기 권장안)

ASR 모델이 "정확도 점수"를 일관되게 제공하지 않는 경우가 많아, 초기에는 아래와 같은 **proxy 지표 기반**으로 충분하다.

- (가능하면) 평균 logprob, no_speech_prob, 디코딩 fallback 여부
- 오디오 길이 대비 토큰 수(비정상적으로 길거나 짧은 경우 플래그)
- 반복 토큰/반복 구문 탐지(모델 붕괴 패턴)
- VAD 기반 "실제 발화 구간"이 너무 짧거나 긴 경우 플래그

초기 운영 전략:
- **보수적으로 B/C를 넉넉히 잡고**, 검수량을 관측하면서 점진적으로 기준을 조정한다.

---

## 6. 리스크 및 완화책

### 리스크 1) ASR 오인식이 정답 라벨로 고정되는 문제
- **완화:** 트리아지 + 선택적 검수(특히 C 버킷은 반드시 확인)
- `text_raw`와 `text_verified` 분리로, 무엇이 자동 생성이고 무엇이 사람이 확정했는지 명확히 한다.

### 리스크 2) 정규화가 의미를 바꾸는 문제(비가역 변환)
- **완화:** 정규화는 `text_verified → text_normalized`의 단방향 파이프라인으로 고정하고, 항상 원문(`text_verified`)을 보존한다.
- 정규화 규칙은 테스트 케이스 기반으로 확장(3.5~3.6 검증 단계).

### 리스크 3) 파인튜닝 여부 결정의 혼선
- **완화:** 3.1 단계에서는 파인튜닝을 "결정"하지 않는다.
  트리아지 결과(B/C 비율, 오류 패턴)를 근거로, 필요할 때 3.x 후반부에서 옵션으로 검토한다.

---

## 7. 3.1 단계 산출물(Deliverables)

3.1 완료 기준은 아래 3가지를 레포에 남기는 것이다.

1) **워크플로우 결정 문서(본 문서)**
2) **라벨 스키마 정의**
   - sample_id 기준: `text_raw`, `text_verified`, `text_normalized`, `triage_bucket`, `triage_score`, `notes`
3) **최소 파이프라인 실행 계획**
   - ASR 엔진/버전/파라미터 고정
   - 트리아지 기준 초안
   - 검수 운영 방식(버킷 B/C 중심)

---

## 8. 다음 단계(3.2로의 연결)

- 3.2에서는 한국어 라벨링/정규화에서 빈번히 발생하는 도메인 이슈(숫자, 단위, 띄어쓰기, 고유명사, 영어/약어 혼용 등)를 정리하고,
- 3.3~3.6에서 정규화 모듈을 강화하고, 가능한 엣지케이스까지 자동 검증 체계를 확장한다.

---

## 부록: 왜 Forced Alignment를 3.1 베이스에서 제외했는가
Forced Alignment는 "텍스트가 존재할 때" 음성-텍스트를 시간축에 정렬하는 데 강력하지만, 3.1의 핵심 과제는 Train/Dev의 **텍스트 자체를 생성**하는 것이므로, 정렬 도구는 **후속 단계(예: 단어 단위 타임스탬프가 필요할 때)**에 선택적으로 도입하는 것이 합리적이라 판단했다.
