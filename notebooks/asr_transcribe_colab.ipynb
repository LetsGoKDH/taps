{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASR Transcription for TAPS Dataset\n",
    "\n",
    "faster-whisper를 사용하여 TAPS 데이터셋의 Acoustic 오디오를 전사합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 의존성 설치\n",
    "!pip install -q faster-whisper datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. GPU 확인\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 스크립트 작성\n",
    "%%writefile asr_transcribe_1000.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ASR Transcription Script for TAPS Dataset (1000 samples)\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional, Set, Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_triage_features(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"트리아지용 텍스트 피처를 계산합니다.\"\"\"\n",
    "    has_digit = bool(re.search(r'\\d', text))\n",
    "    latin_count = len(re.findall(r'[a-zA-Z]', text))\n",
    "    unit_patterns = [\n",
    "        r'\\d+\\s*(?:cm|mm|m|km|kg|g|mg|ml|L|cc|%|도|원|개|명|번|회|시|분|초|년|월|일)',\n",
    "        r'(?:제|약|총|각)\\s*\\d+',\n",
    "    ]\n",
    "    unit_like_count = sum(\n",
    "        len(re.findall(pattern, text, re.IGNORECASE))\n",
    "        for pattern in unit_patterns\n",
    "    )\n",
    "    return {\n",
    "        \"has_digit\": has_digit,\n",
    "        \"latin_count\": latin_count,\n",
    "        \"unit_like_count\": unit_like_count,\n",
    "    }\n",
    "\n",
    "\n",
    "def find_acoustic_field(dataset) -> str:\n",
    "    \"\"\"데이터셋에서 acoustic 오디오 필드를 찾습니다.\"\"\"\n",
    "    features = dataset.features\n",
    "    candidates = [\"Acoustic_Microphone\", \"acoustic_microphone\", \"Acoustic\", \"acoustic\", \"audio\"]\n",
    "    for name in candidates:\n",
    "        if name in features:\n",
    "            return name\n",
    "    for name in features:\n",
    "        if \"acoustic\" in name.lower():\n",
    "            return name\n",
    "    from datasets import Audio\n",
    "    for name, feat in features.items():\n",
    "        if isinstance(feat, Audio):\n",
    "            return name\n",
    "    raise ValueError(f\"Acoustic audio field not found. Available fields: {list(features.keys())}\")\n",
    "\n",
    "\n",
    "def load_done_set(jsonl_path: str) -> Set[str]:\n",
    "    \"\"\"이미 처리된 utt_id 집합을 로드합니다.\"\"\"\n",
    "    done = set()\n",
    "    if os.path.exists(jsonl_path):\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        record = json.loads(line)\n",
    "                        if \"utt_id\" in record:\n",
    "                            done.add(record[\"utt_id\"])\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "    return done\n",
    "\n",
    "\n",
    "def transcribe_audio(\n",
    "    model,\n",
    "    audio_array: np.ndarray,\n",
    "    sample_rate: int,\n",
    "    language: str = \"ko\",\n",
    "    beam_size: int = 5,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"오디오를 전사하고 메타데이터를 반환합니다.\"\"\"\n",
    "    if sample_rate != 16000:\n",
    "        try:\n",
    "            import librosa\n",
    "            audio_array = librosa.resample(audio_array, orig_sr=sample_rate, target_sr=16000)\n",
    "        except ImportError:\n",
    "            from scipy import signal\n",
    "            num_samples = int(len(audio_array) * 16000 / sample_rate)\n",
    "            audio_array = signal.resample(audio_array, num_samples)\n",
    "\n",
    "    segments, info = model.transcribe(\n",
    "        audio_array,\n",
    "        language=language,\n",
    "        beam_size=beam_size,\n",
    "        temperature=[0.0, 0.2, 0.4],\n",
    "        vad_filter=True,\n",
    "    )\n",
    "    segments = list(segments)\n",
    "\n",
    "    if len(segments) == 0:\n",
    "        return {\n",
    "            \"text_raw\": \"\",\n",
    "            \"avg_logprob\": -1.0,\n",
    "            \"avg_no_speech_prob\": 1.0,\n",
    "            \"compression_ratio\": 0.0,\n",
    "            \"language\": language,\n",
    "            \"duration\": info.duration,\n",
    "            \"temperature_fallback\": False,\n",
    "        }\n",
    "\n",
    "    text_raw = \"\".join(seg.text for seg in segments).strip()\n",
    "    total_duration = sum(seg.end - seg.start for seg in segments)\n",
    "\n",
    "    if total_duration > 0:\n",
    "        avg_logprob = sum(seg.avg_logprob * (seg.end - seg.start) for seg in segments) / total_duration\n",
    "        avg_no_speech_prob = sum(seg.no_speech_prob * (seg.end - seg.start) for seg in segments) / total_duration\n",
    "        compression_ratio = sum(seg.compression_ratio * (seg.end - seg.start) for seg in segments) / total_duration\n",
    "    else:\n",
    "        avg_logprob = sum(seg.avg_logprob for seg in segments) / len(segments)\n",
    "        avg_no_speech_prob = sum(seg.no_speech_prob for seg in segments) / len(segments)\n",
    "        compression_ratio = sum(seg.compression_ratio for seg in segments) / len(segments)\n",
    "\n",
    "    temperature_fallback = any(getattr(seg, \"temperature\", 0.0) > 0.0 for seg in segments)\n",
    "\n",
    "    return {\n",
    "        \"text_raw\": text_raw,\n",
    "        \"avg_logprob\": avg_logprob,\n",
    "        \"avg_no_speech_prob\": avg_no_speech_prob,\n",
    "        \"compression_ratio\": compression_ratio,\n",
    "        \"language\": info.language or language,\n",
    "        \"duration\": info.duration,\n",
    "        \"temperature_fallback\": temperature_fallback,\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"ASR transcription for TAPS dataset\")\n",
    "    parser.add_argument(\"--out_jsonl\", type=str, default=\"asr_results.jsonl\")\n",
    "    parser.add_argument(\"--model_size\", type=str, default=\"large-v3\")\n",
    "    parser.add_argument(\"--beam_size\", type=int, default=5)\n",
    "    parser.add_argument(\"--max_items\", type=int, default=None)\n",
    "    parser.add_argument(\"--resume\", action=\"store_true\", default=True)\n",
    "    parser.add_argument(\"--no-resume\", action=\"store_false\", dest=\"resume\")\n",
    "    parser.add_argument(\"--flush_every\", type=int, default=10)\n",
    "    parser.add_argument(\"--device\", type=str, default=\"auto\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    out_path = Path(args.out_jsonl)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    done_set: Set[str] = set()\n",
    "    if args.resume and out_path.exists():\n",
    "        done_set = load_done_set(str(out_path))\n",
    "        print(f\"Resuming: {len(done_set)} items already processed\")\n",
    "\n",
    "    print(\"Loading dataset...\")\n",
    "    from datasets import load_dataset\n",
    "    dataset = load_dataset(\n",
    "        \"yskim3271/Throat_and_Acoustic_Pairing_Speech_Dataset\",\n",
    "        \"with_normalized_text\",\n",
    "        split=\"test\",\n",
    "    )\n",
    "\n",
    "    acoustic_field = find_acoustic_field(dataset)\n",
    "    print(f\"Found acoustic field: {acoustic_field}\")\n",
    "\n",
    "    total_items = len(dataset)\n",
    "    if args.max_items:\n",
    "        total_items = min(total_items, args.max_items)\n",
    "    print(f\"Total items to process: {total_items}\")\n",
    "\n",
    "    print(f\"Loading Whisper model: {args.model_size}\")\n",
    "    from faster_whisper import WhisperModel\n",
    "\n",
    "    if args.device == \"auto\":\n",
    "        try:\n",
    "            import torch\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        except ImportError:\n",
    "            device = \"cpu\"\n",
    "    else:\n",
    "        device = args.device\n",
    "\n",
    "    compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n",
    "    print(f\"Using device: {device}, compute_type: {compute_type}\")\n",
    "\n",
    "    model = WhisperModel(args.model_size, device=device, compute_type=compute_type)\n",
    "    print(\"Model loaded!\")\n",
    "\n",
    "    processed = 0\n",
    "    skipped = 0\n",
    "    errors = 0\n",
    "    buffer: List[Dict[str, Any]] = []\n",
    "\n",
    "    with open(out_path, \"a\", encoding=\"utf-8\") as f_out:\n",
    "        for idx, sample in enumerate(dataset):\n",
    "            if idx >= total_items:\n",
    "                break\n",
    "\n",
    "            speaker_id = str(sample.get(\"speaker_id\", sample.get(\"Speaker_ID\", f\"S{idx:04d}\")))\n",
    "            sentence_id = str(sample.get(\"sentence_id\", sample.get(\"Sentence_ID\", f\"{idx:06d}\")))\n",
    "            utt_id = f\"{speaker_id}_{sentence_id}\"\n",
    "\n",
    "            if utt_id in done_set:\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                audio_data = sample[acoustic_field]\n",
    "                if isinstance(audio_data, dict):\n",
    "                    if \"array\" in audio_data:\n",
    "                        audio_array = np.array(audio_data[\"array\"])\n",
    "                        sample_rate = audio_data.get(\"sampling_rate\", 16000)\n",
    "                    elif \"acoustic_microphone\" in audio_data:\n",
    "                        audio_array = np.array(audio_data[\"acoustic_microphone\"][\"array\"])\n",
    "                        sample_rate = audio_data[\"acoustic_microphone\"].get(\"sampling_rate\", 16000)\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unknown audio format: {audio_data.keys()}\")\n",
    "                else:\n",
    "                    audio_array = np.array(audio_data)\n",
    "                    sample_rate = 16000\n",
    "            except Exception as e:\n",
    "                print(f\"[{idx}] Error extracting audio for {utt_id}: {e}\")\n",
    "                errors += 1\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                result = transcribe_audio(model, audio_array, sample_rate, language=\"ko\", beam_size=args.beam_size)\n",
    "            except Exception as e:\n",
    "                print(f\"[{idx}] Error transcribing {utt_id}: {e}\")\n",
    "                errors += 1\n",
    "                continue\n",
    "\n",
    "            triage_feat = compute_triage_features(result[\"text_raw\"])\n",
    "\n",
    "            record = {\n",
    "                \"utt_id\": utt_id,\n",
    "                \"speaker_id\": speaker_id,\n",
    "                \"sentence_id\": sentence_id,\n",
    "                \"audio_source\": {\n",
    "                    \"dataset\": \"yskim3271/Throat_and_Acoustic_Pairing_Speech_Dataset\",\n",
    "                    \"split\": \"test\",\n",
    "                    \"field\": acoustic_field,\n",
    "                },\n",
    "                \"text_raw\": result[\"text_raw\"],\n",
    "                \"avg_logprob\": result[\"avg_logprob\"],\n",
    "                \"avg_no_speech_prob\": result[\"avg_no_speech_prob\"],\n",
    "                \"compression_ratio\": result[\"compression_ratio\"],\n",
    "                \"temperature_fallback\": result[\"temperature_fallback\"],\n",
    "                \"language\": result[\"language\"],\n",
    "                \"duration\": result[\"duration\"],\n",
    "                \"triage_feat\": triage_feat,\n",
    "            }\n",
    "\n",
    "            buffer.append(record)\n",
    "            processed += 1\n",
    "\n",
    "            if processed % 10 == 0:\n",
    "                print(f\"Processed: {processed}, Skipped: {skipped}, Errors: {errors}\")\n",
    "\n",
    "            if len(buffer) >= args.flush_every:\n",
    "                for rec in buffer:\n",
    "                    f_out.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "                f_out.flush()\n",
    "                buffer.clear()\n",
    "\n",
    "        if buffer:\n",
    "            for rec in buffer:\n",
    "                f_out.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "            f_out.flush()\n",
    "\n",
    "    print(f\"\\nDone!\")\n",
    "    print(f\"  Processed: {processed}\")\n",
    "    print(f\"  Skipped (already done): {skipped}\")\n",
    "    print(f\"  Errors: {errors}\")\n",
    "    print(f\"  Output: {out_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 전사 실행 (50개 샘플)\n",
    "!python asr_transcribe_1000.py --out_jsonl asr_results.jsonl --max_items 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 결과 확인 (20줄)\n",
    "!head -20 asr_results.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 분포 확인\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "records = []\n",
    "with open('asr_results.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        records.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(\"=== 기본 통계 ===\")\n",
    "print(df[['avg_logprob', 'avg_no_speech_prob', 'compression_ratio', 'duration']].describe())\n",
    "\n",
    "print(\"\\n=== Temperature Fallback 발생 ===\")\n",
    "print(df['temperature_fallback'].value_counts())\n",
    "\n",
    "print(\"\\n=== Triage Features 분포 ===\")\n",
    "triage_df = pd.json_normalize(df['triage_feat'])\n",
    "print(f\"has_digit: {triage_df['has_digit'].sum()} / {len(triage_df)}\")\n",
    "print(f\"latin_count > 0: {(triage_df['latin_count'] > 0).sum()} / {len(triage_df)}\")\n",
    "print(f\"unit_like_count > 0: {(triage_df['unit_like_count'] > 0).sum()} / {len(triage_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 파일 다운로드\n",
    "from google.colab import files\n",
    "files.download('asr_results.jsonl')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
